{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import custom_vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image_as_batch_with_optional_resize(path, newH=None, newW=None):\n",
    "    img = skimage.io.imread(path)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    oldH, oldW = img.shape[0], img.shape[1] # assume h,w is shape[0] and [1] respectively\n",
    "    if newH != None or newW != None:\n",
    "        if newW is None:\n",
    "            newW = int(oldW * float(newH) / oldH)\n",
    "        elif newH is None:\n",
    "            newH = int(oldH * float(newW) / oldW)\n",
    "        img = skimage.transform.resize(img, (newH, newW))\n",
    "    \n",
    "    # delete the Alpha channel if the image is RGBA to make sure # channel is correct\n",
    "    if img.shape[2]==4:\n",
    "        img = img[:,:,0:3]\n",
    "        \n",
    "    # add another dimension to make it a batch , bacause our vgg19 def takes a batch\n",
    "    img = img.reshape((1,)+img.shape)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONTENT_LAYER = 'conv3_2'\n",
    "STYLE_LAYERS = ('conv1_1', 'conv2_1', 'conv3_1', 'conv4_1')\n",
    "\n",
    "class ARG:pass\n",
    "arg = ARG()\n",
    "arg.gen_img_height=500\n",
    "\n",
    "styleimg = load_image_as_batch_with_optional_resize('./styles/picasso_selfport1907.jpg')\n",
    "print(styleimg.shape)\n",
    "# contentimg = load_image_as_batch_with_optional_resize('./brad_pitt.jpg', newH=arg.gen_img_height)\n",
    "contentimg = load_image_as_batch_with_optional_resize('contents/smalltestset/COCO_test2014_000000000090.jpg', 256,256)\n",
    "print(contentimg.shape)\n",
    "\n",
    "arg.gen_img_width = contentimg.shape[1] # computed from aspect ratio of content_img\n",
    "# show image\n",
    "skimage.io.imshow(contentimg[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "img_pl = tf.placeholder(tf.float32)\n",
    "vgg19factory = custom_vgg19.Vgg19Factory()\n",
    "vgg19 = vgg19factory.build(img_pl)\n",
    "\n",
    "print(styleimg.shape)\n",
    "conv31feat = sess.run(vgg19.conv3_1, feed_dict={img_pl:styleimg})\n",
    "print(conv31feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(feat_map):\n",
    "    assert isinstance(feat_map, tf.Tensor)\n",
    "    shape = tf.cast(tf.shape(feat_map), tf.float32)\n",
    "    _, h, w, ch = shape[0], shape[1], shape[2], shape[3]\n",
    "    F = tf.reshape(feat_map, [-1, tf.cast(ch,tf.int32)])\n",
    "    \n",
    "    # TODO: if m<n, compute feat_map*feat_map, else compute feat_map'*feat_map \n",
    "    gram = tf.matmul(F, F, transpose_a=True) / h / w / ch # not sure why  we have\"/ ch\". if not, the style_loss is too big\n",
    "    return gram\n",
    "\n",
    "def compute_style_loss(gram_of_feat_map1, feat_map2):\n",
    "    shape = tf.cast(tf.shape(feat_map2), tf.float32)\n",
    "    _, h, w, ch = shape[0], shape[1], shape[2], shape[3]\n",
    "    \n",
    "    G1, G2 = gram_of_feat_map1, gram_matrix(feat_map2)\n",
    "    style_loss = tf.nn.l2_loss((G1-G2))/ (ch**2) # ch^2 is #element in G1, G2\n",
    "    return style_loss\n",
    "\n",
    "def compute_content_loss(feat_map1, feat_map2):\n",
    "    shape = tf.cast(tf.shape(feat_map1), tf.float32)\n",
    "    _, h, w, ch = shape[0], shape[1], shape[2], shape[3]\n",
    "    \n",
    "    content_loss = tf.nn.l2_loss(feat_map1-feat_map2)/h/w/ch\n",
    "    return content_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_shape = contentimg.shape\n",
    "img_gen = tf.Variable(tf.truncated_normal(contentimg.shape,  mean=0.5, stddev=0.1))\n",
    "vgg19_pred= vgg19factory.build(img_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# precompute content/style image's stuffs\n",
    "style_layers_target = [getattr(vgg19, l) for l in STYLE_LAYERS]\n",
    "content_layer_target = getattr(vgg19, CONTENT_LAYER)\n",
    "styleimg_grams_np = [sess.run(gram_matrix(l), feed_dict={img_pl:styleimg}) for l in style_layers_target]\n",
    "contentimg_feat_map_np = sess.run(content_layer_target, feed_dict={img_pl:contentimg})\n",
    "styleimg_grams = [tf.constant(g, dtype=tf.float32) for g in styleimg_grams_np]\n",
    "contentimg_feat_map = tf.constant(contentimg_feat_map_np, dtype=tf.float32)\n",
    "\n",
    "style_layers_pred = [getattr(vgg19_pred, name) for name in STYLE_LAYERS]\n",
    "content_layer_pred = getattr(vgg19_pred, CONTENT_LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(img_gen.initializer)\n",
    "\n",
    "style_losses = [compute_style_loss(src, dst) for (src,dst) in zip(styleimg_grams,style_layers_pred)]\n",
    "style_loss_np = sess.run(style_losses)\n",
    "print 'initial style loss: ' + str(style_loss_np)\n",
    "\n",
    "content_loss = compute_content_loss(contentimg_feat_map, content_layer_pred)\n",
    "[content_loss_np] =sess.run([content_loss])\n",
    "print('initial content loss = %f' % content_loss_np)\n",
    "\n",
    "def _tensor_size(tensor):\n",
    "    from operator import mul\n",
    "    return reduce(mul, (d.value for d in tensor.get_shape()[1:]), 1)\n",
    "BATCH_SIZE=1\n",
    "# total variation denoising\n",
    "bsize, h , w, ch = batch_shape\n",
    "y_tv = tf.nn.l2_loss(img_gen[:,1:,:,:] - img_gen[:,:batch_shape[1]-1,:,:])\n",
    "x_tv = tf.nn.l2_loss(img_gen[:,:,1:,:] - img_gen[:,:,:batch_shape[2]-1,:])\n",
    "tv_loss = (x_tv + y_tv)/(bsize*h*w*ch)\n",
    "[tv_loss_np] =sess.run([tv_loss])\n",
    "print('initial TV loss = %f' % tv_loss_np)\n",
    "\n",
    "total_loss =  10 * reduce(tf.add, style_losses) + 1.5 * content_loss + 100000 * tv_loss\n",
    "[total_loss_np] = sess.run([total_loss])\n",
    "print('initial total loss = %f' % total_loss_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = set(tf.all_variables())\n",
    "train_op = tf.train.AdamOptimizer(0.02).minimize(total_loss)\n",
    "#I honestly don't know how else to initialize ADAM in TensorFlow.\n",
    "sess.run(tf.initialize_variables(set(tf.all_variables()) - temp))\n",
    "\n",
    "# start optimization\n",
    "iter = 0\n",
    "MAX_ITER = 200\n",
    "\n",
    "while iter < MAX_ITER:\n",
    "    sess.run(train_op)\n",
    "    iter += 1\n",
    "    if iter % 10 == 0:\n",
    "        print(sess.run([total_loss]+style_losses+ [content_loss, tv_loss]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "img_gen_clipped = tf.clip_by_value(img_gen, 0,1) #  the range of values in generated image will fall out of [0,1].\n",
    "            # If you scale it to [0,1] instead of clipping it to [0,1], the image will look \"grey\"\n",
    "img_gen_np = np.squeeze(sess.run(img_gen_clipped), 0)\n",
    "skimage.io.imshow(img_gen_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
