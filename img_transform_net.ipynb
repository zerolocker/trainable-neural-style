{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "input_shape = [batch_size, 256, 256, 3]\n",
    "ph = tf.placeholder(tf.float32, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build(img, debugSess=None):\n",
    "    assert input_shape == [d.value for d in img.get_shape()]   # now we fix the image size and assume \n",
    "      # img has an already-defined shape. Otherwise Tensorflow will fail to infer the output shape of the layer\n",
    "        # which is very inconvinent for debugging\n",
    "    \n",
    "    # img = tf.pad(img, [[40,40],[40,40]]) # TODO maybe add it later after I finished it. \n",
    "            # But I don't know why fast-style-transfer repo doesn't follow the paper's supp material PDF to\n",
    "            #    (1) have this padding step\n",
    "            #    (2) not use padding in the residual blocks' conv layers\n",
    "            # Maybe I can try to implement it later to see if it can produce better images\n",
    "    conv1 = conv_layer(img, n_in_channel=3, n_out_channel=32, filter_size=9, stride=1, hasRelu=True)\n",
    "    conv2 = conv_layer(conv1, n_in_channel=32, n_out_channel=64, filter_size=3, stride=2, hasRelu=True)\n",
    "    conv3 = conv_layer(conv2, n_in_channel=64, n_out_channel=128, filter_size=3, stride=2, hasRelu=True)\n",
    "    res1 = residual_block(conv3, n_in_channel=128, n_out_channel=128)\n",
    "    res2 = residual_block(res1, n_in_channel=128, n_out_channel=128)\n",
    "    res3 = residual_block(res2, n_in_channel=128, n_out_channel=128)\n",
    "    res4 = residual_block(res3, n_in_channel=128, n_out_channel=128)\n",
    "    res5 = residual_block(res4, n_in_channel=128, n_out_channel=128)\n",
    "    deconv1 = de_conv_layer(res5, n_in_channel=128, n_out_channel=64, filter_size=3, stride=2)\n",
    "    deconv2 = de_conv_layer(deconv1, n_in_channel=64, n_out_channel=32, filter_size=3, stride=2)\n",
    "    convColor = conv_layer(deconv2, n_in_channel=32, n_out_channel=3, filter_size=9, stride=1)\n",
    "    tanh = tf.nn.tanh(convColor) * 150 + 255./2  # TODO: why tanh * 150 ? why + 255/2 ? \n",
    "    \n",
    "    return tanh\n",
    "    \n",
    "def conv_layer(input, n_in_channel, n_out_channel, filter_size, stride, hasRelu=True):\n",
    "    # TODO conv layer without adding bias ( bias is not used in paper either). but I could try it later if time permitted. tf.nn.bias_add\n",
    "    filt = tf.Variable(tf.truncated_normal([filter_size, filter_size, n_in_channel, n_out_channel], stddev=.1))\n",
    "    output = tf.nn.conv2d(input, filt, [1,stride,stride,1], padding='SAME')\n",
    "    output = _instance_norm(output) # TODO read what is instance normalization \n",
    "    if hasRelu:\n",
    "        output = tf.nn.relu(output)\n",
    "    print(\"conv layer, output size: %s\" % ([i.value for i in output.get_shape()]))\n",
    "    return output\n",
    "\n",
    "def de_conv_layer(input, n_in_channel, n_out_channel, filter_size, stride):\n",
    "    filt = tf.Variable(tf.truncated_normal([filter_size, filter_size, n_out_channel, n_in_channel], stddev=.1))\n",
    "    in_shape = [s.value for s in input.get_shape()]\n",
    "    out_shape = [in_shape[0], in_shape[1]*stride, in_shape[2]*stride, n_out_channel]\n",
    "    output = tf.nn.conv2d_transpose(input, filt, output_shape=out_shape, strides=[1,stride, stride, 1])\n",
    "    print(\"deconv layer, output size: %s\" % ([i.value for i in output.get_shape()]))\n",
    "    return output\n",
    "\n",
    "def residual_block(input, n_in_channel, n_out_channel, name=\"n/a\"):\n",
    "    print(\"START residual_block \")\n",
    "    output = conv_layer(input, n_in_channel, n_out_channel, filter_size=3, stride=1, hasRelu=True)\n",
    "    output = conv_layer(output,n_out_channel, n_out_channel, filter_size=3, stride=1, hasRelu=False)\n",
    "    output = input + output\n",
    "    print(\"END residual_block\")\n",
    "    return output\n",
    "    \n",
    "\n",
    "# TODO Read what is instance normalization. The following code is copied, I don't know how it works\n",
    "def _instance_norm(net):\n",
    "    batch, rows, cols, channels = [i.value for i in net.get_shape()]\n",
    "    var_shape = [channels]\n",
    "    mu, sigma_sq = tf.nn.moments(net, [1,2], keep_dims=True)\n",
    "    shift = tf.Variable(tf.zeros(var_shape))\n",
    "    scale = tf.Variable(tf.ones(var_shape))\n",
    "    epsilon = 1e-3\n",
    "    normalized = (net-mu)/(sigma_sq + epsilon)**(.5)\n",
    "    return scale * normalized + shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv layer, output size: [10, 256, 256, 32]\n",
      "conv layer, output size: [10, 128, 128, 64]\n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "START residual_block \n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "END residual_block\n",
      "START residual_block \n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "END residual_block\n",
      "START residual_block \n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "END residual_block\n",
      "START residual_block \n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "END residual_block\n",
      "START residual_block \n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "conv layer, output size: [10, 64, 64, 128]\n",
      "END residual_block\n",
      "deconv layer, output size: [10, 128, 128, 64]\n",
      "deconv layer, output size: [10, 256, 256, 32]\n",
      "conv layer, output size: [10, 256, 256, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_653:0' shape=(10, 256, 256, 3) dtype=float32>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tf.placeholder(tf.float32, input_shape)\n",
    "\n",
    "build(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 1)\n",
      "[[[[ 1.]\n",
      "   [ 1.]\n",
      "   [ 2.]\n",
      "   [ 1.]]\n",
      "\n",
      "  [[ 1.]\n",
      "   [ 1.]\n",
      "   [ 2.]\n",
      "   [ 1.]]\n",
      "\n",
      "  [[ 2.]\n",
      "   [ 2.]\n",
      "   [ 4.]\n",
      "   [ 2.]]\n",
      "\n",
      "  [[ 1.]\n",
      "   [ 1.]\n",
      "   [ 2.]\n",
      "   [ 1.]]]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
