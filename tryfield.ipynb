{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import custom_vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image_as_batch_with_optional_resize(path, newH=None, newW=None):\n",
    "    img = skimage.io.imread(path)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    oldH, oldW = img.shape[0], img.shape[1] # assume h,w is shape[0] and [1] respectively\n",
    "    if newH != None or newW != None:\n",
    "        if newW is None:\n",
    "            newW = int(oldW * float(newH) / oldH)\n",
    "        elif newH is None:\n",
    "            newH = int(oldH * float(newW) / oldW)\n",
    "        img = skimage.transform.resize(img, (newH, newW))\n",
    "    \n",
    "    # delete the Alpha channel if the image is RGBA to make sure # channel is correct\n",
    "    if img.shape[2]==4:\n",
    "        img = img[:,:,0:3]\n",
    "        \n",
    "    # add another dimension to make it a batch , bacause our vgg19 def takes a batch\n",
    "    img = img.reshape((1,)+img.shape)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 600, 468, 3)\n",
      "(1, 500, 395, 3)\n"
     ]
    }
   ],
   "source": [
    "class ARG:pass\n",
    "arg = ARG()\n",
    "arg.gen_img_height=500\n",
    "arg.styl\n",
    "\n",
    "styleimg = load_image_as_batch_with_optional_resize('./picasso_selfport1907.jpg')\n",
    "print(styleimg.shape)\n",
    "contentimg = load_image_as_batch_with_optional_resize('./brad_pitt.jpg', newH=arg.gen_img_height)\n",
    "print(contentimg.shape)\n",
    "\n",
    "arg.gen_img_width = contentimg.shape[1] # computed from aspect ratio of content_img\n",
    "# show image\n",
    "# skimage.io.imshow(contentimg[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/04006/zhuode93/maverick/dlproj2/trainable-neural-style/tensorflow_vgg/vgg19.npy\n",
      "npy file loaded\n",
      "(1, 600, 468, 3)\n",
      "(1, 150, 117, 256)\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "img_pl = tf.placeholder(tf.float32)\n",
    "vgg19factory = custom_vgg19.Vgg19Factory()\n",
    "vgg19 = vgg19factory.build(img_pl)\n",
    "\n",
    "print(styleimg.shape)\n",
    "conv31feat = sess.run(vgg19.conv3_1, feed_dict={img_pl:styleimg})\n",
    "print(conv31feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(feat_map):\n",
    "    assert isinstance(feat_map, tf.Tensor)\n",
    "    shape = tf.cast(tf.shape(feat_map), tf.float32)\n",
    "    _, h, w, ch = shape[0], shape[1], shape[2], shape[3]\n",
    "    F = tf.reshape(feat_map, [-1, tf.cast(ch,tf.int32)])\n",
    "    \n",
    "    # TODO: if m<n, compute feat_map*feat_map, else compute feat_map'*feat_map \n",
    "    gram = tf.matmul(F, F, transpose_a=True) / h / w / ch # not sure why  we have\"/ ch\". if not, the style_loss is too big\n",
    "    return gram\n",
    "\n",
    "def compute_style_loss(gram_of_feat_map1, feat_map2):\n",
    "    shape = tf.cast(tf.shape(feat_map2), tf.float32)\n",
    "    _, h, w, ch = shape[0], shape[1], shape[2], shape[3]\n",
    "    \n",
    "    G1, G2 = gram_of_feat_map1, gram_matrix(feat_map2)\n",
    "    style_loss = tf.nn.l2_loss((G1-G2))/ (ch**2) # ch^2 is #element in G1, G2\n",
    "    return style_loss\n",
    "\n",
    "def compute_content_loss(feat_map1, feat_map2):\n",
    "    shape = tf.cast(tf.shape(feat_map1), tf.float32)\n",
    "    _, h, w, ch = shape[0], shape[1], shape[2], shape[3]\n",
    "    \n",
    "    content_loss = tf.nn.l2_loss(feat_map1-feat_map2)/h/w/ch\n",
    "    return content_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_gen = tf.Variable(tf.truncated_normal(contentimg.shape,  mean=0.5, stddev=0.1))\n",
    "sess.run(img_gen.initializer)\n",
    "\n",
    "vgg19_for_img_gen = vgg19factory.build(img_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial style loss = 379.185913\n",
      "initial content loss = 19484.115234\n",
      "initial total loss = 398670.031250\n"
     ]
    }
   ],
   "source": [
    "# precompute style image's stuffs\n",
    "contentimg_feat_map = tf.Variable(vgg19.conv2_2,validate_shape=False, trainable=False)\n",
    "styleimg_gram = tf.Variable(gram_matrix(vgg19.conv3_1), validate_shape=False, trainable=False)\n",
    "sess.run(contentimg_feat_map.initializer, feed_dict={img_pl:contentimg})\n",
    "sess.run(styleimg_gram.initializer, feed_dict={img_pl:styleimg})\n",
    "\n",
    "style_loss = compute_style_loss(styleimg_gram, vgg19_for_img_gen.conv3_1)\n",
    "[style_loss_np] =sess.run([style_loss])\n",
    "print('initial style loss = %f' % style_loss_np)\n",
    "\n",
    "content_loss = compute_content_loss(contentimg_feat_map, vgg19_for_img_gen.conv2_2)\n",
    "[content_loss_np] =sess.run([content_loss])\n",
    "print('initial content loss = %f' % content_loss_np)\n",
    "\n",
    "total_loss =  1000*style_loss + content_loss\n",
    "[total_loss_np] = sess.run([total_loss])\n",
    "print('initial total loss = %f' % total_loss_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.82177, 11787.033, 13608.803]\n"
     ]
    }
   ],
   "source": [
    "temp = set(tf.all_variables())\n",
    "train_op = tf.train.AdamOptimizer(0.02).minimize(total_loss)\n",
    "#I honestly don't know how else to initialize ADAM in TensorFlow.\n",
    "sess.run(tf.initialize_variables(set(tf.all_variables()) - temp))\n",
    "\n",
    "# start optimization\n",
    "iter = 0\n",
    "MAX_ITER = 200\n",
    "\n",
    "while iter < MAX_ITER:\n",
    "    sess.run(train_op)\n",
    "    iter += 1\n",
    "print(sess.run([style_loss, content_loss, total_loss]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_gen_clipped = tf.clip_by_value(img_gen, 0,1) #  the range of values in generated image will fall out of [0,1].\n",
    "            # If you scale it to [0,1] instead of clipping it to [0,1], the image will look \"grey\"\n",
    "img_gen_np = np.squeeze(sess.run(img_gen_clipped), 0)\n",
    "skimage.io.imshow(img_gen_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 395, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
